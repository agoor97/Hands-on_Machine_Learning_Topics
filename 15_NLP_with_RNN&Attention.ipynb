{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b358f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most Important\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "## less Important\n",
    "from functools import partial\n",
    "import os\n",
    "from scipy import stats\n",
    "import missingno as msno\n",
    "import joblib\n",
    "import tarfile\n",
    "import shutil\n",
    "import urllib\n",
    "\n",
    "## Sklearn\n",
    "from sklearn import datasets\n",
    "## Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## tensorflow & Keras\n",
    "import tensorflow as tf    ## i will use tf for every thing and for keras using tf.keras\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94713362",
   "metadata": {},
   "source": [
    "## In this Notebook:\n",
    "\n",
    "### Stateless RNN \n",
    "* `trained to predict the next charachter in a sentence`\n",
    "* `stateless RNN learns a random portion of text on each Iteration, without any information about the rest of the text`\n",
    "\n",
    "### Statefull RNN\n",
    "* `which preserves the hidden state between training iteration and continue reading where it left off, allowing it to learn longer pattrens`\n",
    "\n",
    "### RNN for Sentiment Analysis\n",
    "* `e.g, reading movies reviews, and extract the rater's feelings about the movie`\n",
    "* `this time treating sentences as sequences if words rather than charchters`\n",
    "\n",
    "### Encoder-Decoder archiecture\n",
    "* `to perform NMT (neural machine translation) using seq2seq API provided by TensorFlow`\n",
    "\n",
    "### Attention Mechanism\n",
    "* `these are neural networks that learn to select the part of the inputs that the rest of the model should focus on each time step, and boost performance of the RNN-encoder-decoder, the we will rop RNN and use attention architecture only called (Transformers)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bf700",
   "metadata": {},
   "source": [
    "### Stateless RNN\n",
    "`trained to predict the next charachter in a sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0221bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the data => using shakespeare\n",
    "shakespeare_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "## take the name on your local pc and url\n",
    "filepath = tf.keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "\n",
    "with open(filepath) as f:       ## reading the file\n",
    "    shakespeare_txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb0a5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenization\n",
    "\n",
    "char_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "char_tokenizer.fit_on_texts(shakespeare_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a4522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_to_seq => [[15, 4, 7, 5, 15, 15, 2, 13, 1, 5, 21, 4, 4, 9]]\n",
      "\n",
      "seq_to_text => ['m o h a m m e d   a g o o r']\n"
     ]
    }
   ],
   "source": [
    "## test yout tokenizer by passing to it a text\n",
    "\n",
    "## from text to sequence\n",
    "text_to_seq = char_tokenizer.texts_to_sequences(['mohammed agoor'])\n",
    "print('text_to_seq =>', text_to_seq)\n",
    "\n",
    "print()\n",
    "\n",
    "## take the above output and get from sequence to text\n",
    "seq_to_text = char_tokenizer.sequences_to_texts(text_to_seq)\n",
    "print('seq_to_text =>', seq_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa29f7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_chars => 39\n",
      "size_dataset => 1115394\n"
     ]
    }
   ],
   "source": [
    "## test yout tokenizer by passing to it a text\n",
    "\n",
    "## from text to sequence\n",
    "max_chars = len(char_tokenizer.word_index)\n",
    "print('max_chars =>', max_chars)\n",
    "\n",
    "size_dataset = char_tokenizer.document_count\n",
    "print('size_dataset =>', size_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05410c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenize from texts to sequences\n",
    "## because the indexing starts from 1, decrease by 1 to start from 0\n",
    "\n",
    "full_dataset = np.array(char_tokenizer.texts_to_sequences([shakespeare_txt])) - 1 \n",
    "full_dataset = np.squeeze(full_dataset)\n",
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de3e5f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([19  5  8  7  2], shape=(5,), dtype=int32)\n",
      "tf.Tensor([ 0 18  5  2  5], shape=(5,), dtype=int32)\n",
      "tf.Tensor([35  1  9 23 10], shape=(5,), dtype=int32)\n",
      "tf.Tensor([21  1 19  3  8], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "##  num_total_chars = 1115394\n",
    "## take 90% for training\n",
    "ratio_train = size_dataset * 90 // 100\n",
    "train_set = full_dataset[:ratio_train]\n",
    "\n",
    "## create tensor dataset\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices(train_set)\n",
    "\n",
    "## show some values\n",
    "for val in train_set.batch(5).take(4):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0bdcb",
   "metadata": {},
   "source": [
    "#### `Chopping the Sequential Dataset into Mulible Windows`\n",
    "\n",
    "* `the train_set now consists of a single sequence with over million chars, so we can not just train model because RNN would be equivalent to a deep net with over million layers`\n",
    "* Instead we we will use dataset's window, to convert this long sequence of chars to small windows of text, every window will be a short substring of this long sequence\n",
    "* `but what is the length of this substring, how many chars to used in it, it is easier to train RNN on shorter strings, but RNN will not be able to learn any pattern longer than this length, so don't make it so small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f3b8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x0000016477BF7C40>\n",
      "<tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x0000016477BF7D00>\n",
      "<tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x0000016477BF7C40>\n",
      "<tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x0000016477BF7D00>\n"
     ]
    }
   ],
   "source": [
    "## use n_steps=100\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1  ## shift 1 for target\n",
    "\n",
    "## we will create windows which are not overlapped, but shifted = 1\n",
    "## the first window will be from 0 to 100, second from 1 to 101 and so on\n",
    "## dropping the last windows which do not achieve the window length to have equally windows\n",
    "train_set_window = train_set.window(size=window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "\n",
    "## show some values\n",
    "for val in train_set_window.batch(5).take(4):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b5bf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[19  5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1\n",
      "   0 22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1\n",
      "   4  8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24\n",
      "  17  0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23\n",
      "  10 15  3 13  0]], shape=(1, 101), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 5  8  7  2  0 18  5  2  5 35  1  9 23 10 21  1 19  3  8  1  0 16  1  0\n",
      "  22  8  3 18  1  1 12  0  4  9 15  0 19 13  8  2  6  1  8 17  0  6  1  4\n",
      "   8  0 14  1  0  7 22  1  4 24 26 10 10  4 11 11 23 10  7 22  1  4 24 17\n",
      "   0  7 22  1  4 24 26 10 10 19  5  8  7  2  0 18  5  2  5 35  1  9 23 10\n",
      "  15  3 13  0  4]], shape=(1, 101), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## this output is nested, we should flat_map it\n",
    "## flat_map and the result should be batched for each window \n",
    "## e.g => if the output from the window is {{1,2},{3,4,5,6}}\n",
    "## then after apply this to flat_map (lambda ds:ds.batch(2))\n",
    "## the result will be {[1,2],[3,4],[5,6]}  => this what i want\n",
    "\n",
    "train_set_mapped = train_set_window.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "## show some values\n",
    "for val in train_set_mapped.batch(1).take(2):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43502fb9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features is : ==== \n",
      "tf.Tensor(\n",
      "[[[15 10 16 ...  7  1  0]\n",
      "  [24  1  0 ...  5 18  6]\n",
      "  [12  0 18 ... 20  3 12]\n",
      "  ...\n",
      "  [ 9 23 10 ...  3 13 10]\n",
      "  [ 5  2  5 ...  6  3  9]\n",
      "  [ 0 13  7 ...  7  6  4]]], shape=(1, 32, 100), dtype=int32)\n",
      "================================================================================\n",
      "tf.Tensor(\n",
      "[[[10 16  5 ...  1  0  4]\n",
      "  [ 1  0  1 ... 18  6 17]\n",
      "  [ 0 18 13 ...  3 12  7]\n",
      "  ...\n",
      "  [23 10  2 ... 13 10  5]\n",
      "  [ 2  5 35 ...  3  9  1]\n",
      "  [13  7 26 ...  6  4 11]]], shape=(1, 32, 100), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## shuffle and batch_size of 32 (each batch contains 32 window, each window=101 chars)\n",
    "\n",
    "use_batch_size = 32\n",
    "\n",
    "## each batch consists of 32 window, each window with 101 chars\n",
    "train_set_shuffled = train_set_mapped.shuffle(buffer_size=10000).batch(use_batch_size)\n",
    "train_set_splitted = train_set_shuffled.map(lambda window: (window[:, :-1], window[:, 1:]))\n",
    "\n",
    "## show some values\n",
    "for feat, target in train_set_splitted.batch(1).take(1): \n",
    "    print('Features is : ==== ')\n",
    "    print(feat)\n",
    "    print('===='*20)\n",
    "    print(target)   ## Looks very GREAT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bacf80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features is : ==== \n",
      "tf.Tensor(\n",
      "[[[[1. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 1. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 1. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 1. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 1. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 1. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [1. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 1. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 1. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]], shape=(1, 32, 100, 39), dtype=float32)\n",
      "================================================================================\n",
      "tf.Tensor(\n",
      "[[[16  1  0 ...  1 15  0]\n",
      "  [ 2  5 35 ... 10  4  9]\n",
      "  [ 6  4  2 ...  3 13  8]\n",
      "  ...\n",
      "  [10 19  5 ...  7  2  0]\n",
      "  [ 0  2  3 ...  6  1  0]\n",
      "  [ 3 13  7 ... 14  3  7]]], shape=(1, 32, 100), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## we must encode these IDS => I will use ONE HOT encoding\n",
    "train_set_encoded = train_set_splitted.map(lambda X, y: (tf.one_hot(X, depth=max_chars), y))\n",
    "\n",
    "for feat, target in train_set_encoded.batch(1).take(1): \n",
    "    print('Features is : ==== ')\n",
    "    print(feat)\n",
    "    print('===='*20)\n",
    "    print(target)   ## Looks very GREAT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d3e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## finally prefetch\n",
    "train_set_final = train_set_encoded.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a4cf5",
   "metadata": {},
   "source": [
    "#### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95302193",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "31368/31368 [==============================] - 654s 21ms/step - loss: 1.6169 - accuracy: 0.5059\n",
      "Epoch 2/3\n",
      "31368/31368 [==============================] - 666s 21ms/step - loss: 1.5330 - accuracy: 0.5269\n",
      "Epoch 3/3\n",
      "31368/31368 [==============================] - 659s 21ms/step - loss: 1.5118 - accuracy: 0.5324\n"
     ]
    }
   ],
   "source": [
    "model_char = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2, \n",
    "                        input_shape=[None, max_chars]),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_chars, activation='softmax'))\n",
    "])\n",
    "\n",
    "model_char.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model_char.fit(train_set_final, epochs=3)  ## need more training (saving my time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90f88c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_char.save('model_char_stateless_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "decf3340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### define a function to process the new data\n",
    "def preprocess(texts):\n",
    "    x = np.array(char_tokenizer.texts_to_sequences(texts)) - 1\n",
    "    x = tf.one_hot(x, depth=max_chars)\n",
    "    return x \n",
    "    \n",
    "    \n",
    "X_new = preprocess(['what is your nam'])\n",
    "y_pred = np.argmax(model_char.predict(X_new), axis=-1)\n",
    "\n",
    "char_tokenizer.sequences_to_texts(y_pred + 1)[0][-1]    ## add 1 which you decreases before\n",
    "\n",
    "## Great work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81876e1",
   "metadata": {},
   "source": [
    "#### `Generating Fake Text`\n",
    "\n",
    "* `to generate next text using char-RNN stateless model, we could feed it some test, make the model predict the most likely next char, add it to the whole text and so on`\n",
    "* but in practice, this often leads to the same words being repeated over and over again\n",
    "* `Instead we can pick the next char randomly with a probability equal to the estimated probability`\n",
    "* using tf.random.categorical => samples random class indecies, given the class log probabilities (logits), and to have more control over diversity of the generated text, we can divide the logits by a number called (temperature)\n",
    "* `A temperature close to 0 will favor the high probabilty chars and very high temperature will give all chars an equal probabilty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abd0c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1.0):\n",
    "    ''' this function predicts the next char and transform it to text\n",
    "    '''\n",
    "    X_new = preprocess(text)\n",
    "    y_proba = model_char.predict(X_new)[0,-1:,:]\n",
    "    y_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(logits=y_logits, num_samples=1) + 1 \n",
    "    ## as we decrease 1 in preprocss\n",
    "    char_next = char_tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "    return char_next\n",
    "\n",
    "\n",
    "## the above function get the next char, we want another function to add this preidcted char to\n",
    "## the whole text and so on\n",
    "\n",
    "def complete_char(text, temperature=1.0, n_char=50):  ## required n_char\n",
    "    ''' this function is to concatenate the predicted char with the whole text\n",
    "    '''\n",
    "    for i in range(n_char):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1318103b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t ho heifrh r\\ne!ho  e ooh uloeho, h hsiyw,hhe  iao '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_char('t', temperature=1.0, n_char=50)  ## need more training and tuning (save my time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf85df",
   "metadata": {},
   "source": [
    "-------\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ef0ce",
   "metadata": {},
   "source": [
    "### Statefull RNN\n",
    "* `At stateless RNN, the model starts with the hidden state full of zeros then it updates this state at each time step and after the last time step it throws it away`\n",
    "* `what if we told the RNN to preserve the final hidden state after processing on each training batch and use it as the initial state for the next batch (not the next batch in the same epoch), but each epoch is isolated from each other`\n",
    "* `each batch starts where the previous batch stopped, so not overlapping or shuffling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32410ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  num_total_chars = 1115394\n",
    "## take 90% for training\n",
    "ratio_train = size_dataset * 90 // 100   ## to avoid any fraction\n",
    "train_set_2 = full_dataset[:ratio_train]\n",
    "\n",
    "train_set_2 = tf.data.Dataset.from_tensor_slices(train_set_2)\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "\n",
    "\n",
    "## the same steps as previous\n",
    "train_set_window_2 = train_set_2.window(size=window_length, shift=n_steps, drop_remainder=True)\n",
    "train_set_mapped_2 = train_set_window_2.flat_map(lambda window: window.batch(window_length))\n",
    "train_set_mapped_2 = train_set_mapped_2.batch(1)\n",
    "train_set_divided_2 = train_set_mapped_2.map(lambda window: (window[:, :-1], window[:, 1:]))\n",
    "train_set_encoded_2 = train_set_divided_2.map(lambda x, y: (tf.one_hot(x, depth=max_chars), y))\n",
    "train_set_final_2 = train_set_encoded_2.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bbd339",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "Note : `stateful=True`, `and you must specify the (batch_size) in the batch_input_shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7eb065f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_batch = 1\n",
    "model_char_2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, stateful=True, \n",
    "                       batch_input_shape=[use_batch, None, max_chars]),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_chars, activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "386ef6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hidden states in passed from one batch to another in the same epoch\n",
    "## but after the epoch is done we start the next epoch with zero hidden states and so on\n",
    "class ResetStatesCallback(tf.keras.callbacks.Callback):  ## put it in the callbacks in fit\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bfc0909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10038/10038 [==============================] - 145s 14ms/step - loss: 1.7172 - accuracy: 0.4865\n",
      "Epoch 2/3\n",
      "10038/10038 [==============================] - 142s 14ms/step - loss: 1.4555 - accuracy: 0.5536\n",
      "Epoch 3/3\n",
      "10038/10038 [==============================] - 146s 15ms/step - loss: 1.3878 - accuracy: 0.5712\n"
     ]
    }
   ],
   "source": [
    "model_char_2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model_char_2.fit(train_set_final_2, epochs=3, \n",
    "                           callbacks=[ResetStatesCallback()])  \n",
    "\n",
    "## model need to be trained for many epochs (save my time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74aeedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_char_2.save('model_char_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281464a2",
   "metadata": {},
   "source": [
    "* `Note that you can not use this statefull model with diffrent batches, Insted we make a copy as stateless one and take the weights from statefull one, as I will do`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60805277",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a stateless model with the same structure\n",
    "stateless_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_chars]),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_chars, activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84881fc",
   "metadata": {},
   "source": [
    "#### `To set the weights, we first need to build the model (so the weights get created):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc87d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_chars]))  ## must build first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f54566eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model_char_2.get_weights())  ## set the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66857c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1.0):\n",
    "    X_new = preprocess(text)\n",
    "    y_proba = stateless_model.predict(X_new)[0,-1:,:]  ## change the model to > stateless_model\n",
    "    y_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(logits=y_logits, num_samples=1) + 1 \n",
    "    ## as we decrease 1 in preprocss\n",
    "    char_next = char_tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "    return char_next\n",
    "\n",
    "\n",
    "## the above function get the next char, we want another function to add this preidcted char to\n",
    "## the whole text and so on\n",
    "\n",
    "def complete_char(text, temperature=1.0, n_char=50):  ## required n_char\n",
    "    for i in range(n_char):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5500c6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3tsl:\\nj,:y y,h,s\\n:?h o:?;,\\n\\na\\n.:s wau:a- r .e::ez;'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_char('t', temperature=2, n_char=50)  ## model need to be trained and tuned (save my time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338387a",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae681b6",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "`imdb reviews dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8a762ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_full shape (25000,)\n",
      "y_train_full shape (25000,)\n",
      "X_test shape (25000,)\n",
      "y_test shape (25000,)\n"
     ]
    }
   ],
   "source": [
    "## loading the imdb data\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.imdb.load_data()\n",
    "\n",
    "print('X_train_full shape', X_train_full.shape)\n",
    "print('y_train_full shape', y_train_full.shape)\n",
    "print('X_test shape', X_test.shape)\n",
    "print('y_test shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73b60790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]"
     ]
    }
   ],
   "source": [
    "## get insid the data\n",
    "print(X_train_full[0], end='')  ## it is word indexed not char indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7cabc5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_idx_word => 88586\n"
     ]
    }
   ],
   "source": [
    "## get the max word indexed in the data\n",
    "max_idx_word = max([max(sequence) for sequence in X_train_full])\n",
    "print('max_idx_word =>', max_idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f2a9a34",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34701,\n",
       " 'tsukino': 52006,\n",
       " 'nunnery': 52007,\n",
       " 'sonja': 16816,\n",
       " 'vani': 63951,\n",
       " 'woods': 1408,\n",
       " 'spiders': 16115,\n",
       " 'hanging': 2345,\n",
       " 'woody': 2289,\n",
       " 'trawling': 52008,\n",
       " \"hold's\": 52009,\n",
       " 'comically': 11307,\n",
       " 'localized': 40830,\n",
       " 'disobeying': 30568,\n",
       " \"'royale\": 52010,\n",
       " \"harpo's\": 40831,\n",
       " 'canet': 52011,\n",
       " 'aileen': 19313,\n",
       " 'acurately': 52012,\n",
       " \"diplomat's\": 52013,\n",
       " 'rickman': 25242,\n",
       " 'arranged': 6746,\n",
       " 'rumbustious': 52014,\n",
       " 'familiarness': 52015,\n",
       " \"spider'\": 52016,\n",
       " 'hahahah': 68804,\n",
       " \"wood'\": 52017,\n",
       " 'transvestism': 40833,\n",
       " \"hangin'\": 34702,\n",
       " 'bringing': 2338,\n",
       " 'seamier': 40834,\n",
       " 'wooded': 34703,\n",
       " 'bravora': 52018,\n",
       " 'grueling': 16817,\n",
       " 'wooden': 1636,\n",
       " 'wednesday': 16818,\n",
       " \"'prix\": 52019,\n",
       " 'altagracia': 34704,\n",
       " 'circuitry': 52020,\n",
       " 'crotch': 11585,\n",
       " 'busybody': 57766,\n",
       " \"tart'n'tangy\": 52021,\n",
       " 'burgade': 14129,\n",
       " 'thrace': 52023,\n",
       " \"tom's\": 11038,\n",
       " 'snuggles': 52025,\n",
       " 'francesco': 29114,\n",
       " 'complainers': 52027,\n",
       " 'templarios': 52125,\n",
       " '272': 40835,\n",
       " '273': 52028,\n",
       " 'zaniacs': 52130,\n",
       " '275': 34706,\n",
       " 'consenting': 27631,\n",
       " 'snuggled': 40836,\n",
       " 'inanimate': 15492,\n",
       " 'uality': 52030,\n",
       " 'bronte': 11926,\n",
       " 'errors': 4010,\n",
       " 'dialogs': 3230,\n",
       " \"yomada's\": 52031,\n",
       " \"madman's\": 34707,\n",
       " 'dialoge': 30585,\n",
       " 'usenet': 52033,\n",
       " 'videodrome': 40837,\n",
       " \"kid'\": 26338,\n",
       " 'pawed': 52034,\n",
       " \"'girlfriend'\": 30569,\n",
       " \"'pleasure\": 52035,\n",
       " \"'reloaded'\": 52036,\n",
       " \"kazakos'\": 40839,\n",
       " 'rocque': 52037,\n",
       " 'mailings': 52038,\n",
       " 'brainwashed': 11927,\n",
       " 'mcanally': 16819,\n",
       " \"tom''\": 52039,\n",
       " 'kurupt': 25243,\n",
       " 'affiliated': 21905,\n",
       " 'babaganoosh': 52040,\n",
       " \"noe's\": 40840,\n",
       " 'quart': 40841,\n",
       " 'kids': 359,\n",
       " 'uplifting': 5034,\n",
       " 'controversy': 7093,\n",
       " 'kida': 21906,\n",
       " 'kidd': 23379,\n",
       " \"error'\": 52041,\n",
       " 'neurologist': 52042,\n",
       " 'spotty': 18510,\n",
       " 'cobblers': 30570,\n",
       " 'projection': 9878,\n",
       " 'fastforwarding': 40842,\n",
       " 'sters': 52043,\n",
       " \"eggar's\": 52044,\n",
       " 'etherything': 52045,\n",
       " 'gateshead': 40843,\n",
       " 'airball': 34708,\n",
       " 'unsinkable': 25244,\n",
       " 'stern': 7180,\n",
       " \"cervi's\": 52046,\n",
       " 'dnd': 40844,\n",
       " 'dna': 11586,\n",
       " 'insecurity': 20598,\n",
       " \"'reboot'\": 52047,\n",
       " 'trelkovsky': 11037,\n",
       " 'jaekel': 52048,\n",
       " 'sidebars': 52049,\n",
       " \"sforza's\": 52050,\n",
       " 'distortions': 17633,\n",
       " 'mutinies': 52051,\n",
       " 'sermons': 30602,\n",
       " '7ft': 40846,\n",
       " 'boobage': 52052,\n",
       " \"o'bannon's\": 52053,\n",
       " 'populations': 23380,\n",
       " 'chulak': 52054,\n",
       " 'mesmerize': 27633,\n",
       " 'quinnell': 52055,\n",
       " 'yahoo': 10307,\n",
       " 'meteorologist': 52057,\n",
       " 'beswick': 42577,\n",
       " 'boorman': 15493,\n",
       " 'voicework': 40847,\n",
       " \"ster'\": 52058,\n",
       " 'blustering': 22922,\n",
       " 'hj': 52059,\n",
       " 'intake': 27634,\n",
       " 'morally': 5621,\n",
       " 'jumbling': 40849,\n",
       " 'bowersock': 52060,\n",
       " \"'porky's'\": 52061,\n",
       " 'gershon': 16821,\n",
       " 'ludicrosity': 40850,\n",
       " 'coprophilia': 52062,\n",
       " 'expressively': 40851,\n",
       " \"india's\": 19500,\n",
       " \"post's\": 34710,\n",
       " 'wana': 52063,\n",
       " 'wang': 5283,\n",
       " 'wand': 30571,\n",
       " 'wane': 25245,\n",
       " 'edgeways': 52321,\n",
       " 'titanium': 34711,\n",
       " 'pinta': 40852,\n",
       " 'want': 178,\n",
       " 'pinto': 30572,\n",
       " 'whoopdedoodles': 52065,\n",
       " 'tchaikovsky': 21908,\n",
       " 'travel': 2103,\n",
       " \"'victory'\": 52066,\n",
       " 'copious': 11928,\n",
       " 'gouge': 22433,\n",
       " \"chapters'\": 52067,\n",
       " 'barbra': 6702,\n",
       " 'uselessness': 30573,\n",
       " \"wan'\": 52068,\n",
       " 'assimilated': 27635,\n",
       " 'petiot': 16116,\n",
       " 'most\\x85and': 52069,\n",
       " 'dinosaurs': 3930,\n",
       " 'wrong': 352,\n",
       " 'seda': 52070,\n",
       " 'stollen': 52071,\n",
       " 'sentencing': 34712,\n",
       " 'ouroboros': 40853,\n",
       " 'assimilates': 40854,\n",
       " 'colorfully': 40855,\n",
       " 'glenne': 27636,\n",
       " 'dongen': 52072,\n",
       " 'subplots': 4760,\n",
       " 'kiloton': 52073,\n",
       " 'chandon': 23381,\n",
       " \"effect'\": 34713,\n",
       " 'snugly': 27637,\n",
       " 'kuei': 40856,\n",
       " 'welcomed': 9092,\n",
       " 'dishonor': 30071,\n",
       " 'concurrence': 52075,\n",
       " 'stoicism': 23382,\n",
       " \"guys'\": 14896,\n",
       " \"beroemd'\": 52077,\n",
       " 'butcher': 6703,\n",
       " \"melfi's\": 40857,\n",
       " 'aargh': 30623,\n",
       " 'playhouse': 20599,\n",
       " 'wickedly': 11308,\n",
       " 'fit': 1180,\n",
       " 'labratory': 52078,\n",
       " 'lifeline': 40859,\n",
       " 'screaming': 1927,\n",
       " 'fix': 4287,\n",
       " 'cineliterate': 52079,\n",
       " 'fic': 52080,\n",
       " 'fia': 52081,\n",
       " 'fig': 34714,\n",
       " 'fmvs': 52082,\n",
       " 'fie': 52083,\n",
       " 'reentered': 52084,\n",
       " 'fin': 30574,\n",
       " 'doctresses': 52085,\n",
       " 'fil': 52086,\n",
       " 'zucker': 12606,\n",
       " 'ached': 31931,\n",
       " 'counsil': 52088,\n",
       " 'paterfamilias': 52089,\n",
       " 'songwriter': 13885,\n",
       " 'shivam': 34715,\n",
       " 'hurting': 9654,\n",
       " 'effects': 299,\n",
       " 'slauther': 52090,\n",
       " \"'flame'\": 52091,\n",
       " 'sommerset': 52092,\n",
       " 'interwhined': 52093,\n",
       " 'whacking': 27638,\n",
       " 'bartok': 52094,\n",
       " 'barton': 8775,\n",
       " 'frewer': 21909,\n",
       " \"fi'\": 52095,\n",
       " 'ingrid': 6192,\n",
       " 'stribor': 30575,\n",
       " 'approporiately': 52096,\n",
       " 'wobblyhand': 52097,\n",
       " 'tantalisingly': 52098,\n",
       " 'ankylosaurus': 52099,\n",
       " 'parasites': 17634,\n",
       " 'childen': 52100,\n",
       " \"jenkins'\": 52101,\n",
       " 'metafiction': 52102,\n",
       " 'golem': 17635,\n",
       " 'indiscretion': 40860,\n",
       " \"reeves'\": 23383,\n",
       " \"inamorata's\": 57781,\n",
       " 'brittannica': 52104,\n",
       " 'adapt': 7916,\n",
       " \"russo's\": 30576,\n",
       " 'guitarists': 48246,\n",
       " 'abbott': 10553,\n",
       " 'abbots': 40861,\n",
       " 'lanisha': 17649,\n",
       " 'magickal': 40863,\n",
       " 'mattter': 52105,\n",
       " \"'willy\": 52106,\n",
       " 'pumpkins': 34716,\n",
       " 'stuntpeople': 52107,\n",
       " 'estimate': 30577,\n",
       " 'ugghhh': 40864,\n",
       " 'gameplay': 11309,\n",
       " \"wern't\": 52108,\n",
       " \"n'sync\": 40865,\n",
       " 'sickeningly': 16117,\n",
       " 'chiara': 40866,\n",
       " 'disturbed': 4011,\n",
       " 'portmanteau': 40867,\n",
       " 'ineffectively': 52109,\n",
       " \"duchonvey's\": 82143,\n",
       " \"nasty'\": 37519,\n",
       " 'purpose': 1285,\n",
       " 'lazers': 52112,\n",
       " 'lightened': 28105,\n",
       " 'kaliganj': 52113,\n",
       " 'popularism': 52114,\n",
       " \"damme's\": 18511,\n",
       " 'stylistics': 30578,\n",
       " 'mindgaming': 52115,\n",
       " 'spoilerish': 46449,\n",
       " \"'corny'\": 52117,\n",
       " 'boerner': 34718,\n",
       " 'olds': 6792,\n",
       " 'bakelite': 52118,\n",
       " 'renovated': 27639,\n",
       " 'forrester': 27640,\n",
       " \"lumiere's\": 52119,\n",
       " 'gaskets': 52024,\n",
       " 'needed': 884,\n",
       " 'smight': 34719,\n",
       " 'master': 1297,\n",
       " \"edie's\": 25905,\n",
       " 'seeber': 40868,\n",
       " 'hiya': 52120,\n",
       " 'fuzziness': 52121,\n",
       " 'genesis': 14897,\n",
       " 'rewards': 12607,\n",
       " 'enthrall': 30579,\n",
       " \"'about\": 40869,\n",
       " \"recollection's\": 52122,\n",
       " 'mutilated': 11039,\n",
       " 'fatherlands': 52123,\n",
       " \"fischer's\": 52124,\n",
       " 'positively': 5399,\n",
       " '270': 34705,\n",
       " 'ahmed': 34720,\n",
       " 'zatoichi': 9836,\n",
       " 'bannister': 13886,\n",
       " 'anniversaries': 52127,\n",
       " \"helm's\": 30580,\n",
       " \"'work'\": 52128,\n",
       " 'exclaimed': 34721,\n",
       " \"'unfunny'\": 52129,\n",
       " '274': 52029,\n",
       " 'feeling': 544,\n",
       " \"wanda's\": 52131,\n",
       " 'dolan': 33266,\n",
       " '278': 52133,\n",
       " 'peacoat': 52134,\n",
       " 'brawny': 40870,\n",
       " 'mishra': 40871,\n",
       " 'worlders': 40872,\n",
       " 'protags': 52135,\n",
       " 'skullcap': 52136,\n",
       " 'dastagir': 57596,\n",
       " 'affairs': 5622,\n",
       " 'wholesome': 7799,\n",
       " 'hymen': 52137,\n",
       " 'paramedics': 25246,\n",
       " 'unpersons': 52138,\n",
       " 'heavyarms': 52139,\n",
       " 'affaire': 52140,\n",
       " 'coulisses': 52141,\n",
       " 'hymer': 40873,\n",
       " 'kremlin': 52142,\n",
       " 'shipments': 30581,\n",
       " 'pixilated': 52143,\n",
       " \"'00s\": 30582,\n",
       " 'diminishing': 18512,\n",
       " 'cinematic': 1357,\n",
       " 'resonates': 14898,\n",
       " 'simplify': 40874,\n",
       " \"nature'\": 40875,\n",
       " 'temptresses': 40876,\n",
       " 'reverence': 16822,\n",
       " 'resonated': 19502,\n",
       " 'dailey': 34722,\n",
       " '2\\x85': 52144,\n",
       " 'treize': 27641,\n",
       " 'majo': 52145,\n",
       " 'kiya': 21910,\n",
       " 'woolnough': 52146,\n",
       " 'thanatos': 39797,\n",
       " 'sandoval': 35731,\n",
       " 'dorama': 40879,\n",
       " \"o'shaughnessy\": 52147,\n",
       " 'tech': 4988,\n",
       " 'fugitives': 32018,\n",
       " 'teck': 30583,\n",
       " \"'e'\": 76125,\n",
       " 'doesnâ€™t': 40881,\n",
       " 'purged': 52149,\n",
       " 'saying': 657,\n",
       " \"martians'\": 41095,\n",
       " 'norliss': 23418,\n",
       " 'dickey': 27642,\n",
       " 'dicker': 52152,\n",
       " \"'sependipity\": 52153,\n",
       " 'padded': 8422,\n",
       " 'ordell': 57792,\n",
       " \"sturges'\": 40882,\n",
       " 'independentcritics': 52154,\n",
       " 'tempted': 5745,\n",
       " \"atkinson's\": 34724,\n",
       " 'hounded': 25247,\n",
       " 'apace': 52155,\n",
       " 'clicked': 15494,\n",
       " \"'humor'\": 30584,\n",
       " \"martino's\": 17177,\n",
       " \"'supporting\": 52156,\n",
       " 'warmongering': 52032,\n",
       " \"zemeckis's\": 34725,\n",
       " 'lube': 21911,\n",
       " 'shocky': 52157,\n",
       " 'plate': 7476,\n",
       " 'plata': 40883,\n",
       " 'sturgess': 40884,\n",
       " \"nerds'\": 40885,\n",
       " 'plato': 20600,\n",
       " 'plath': 34726,\n",
       " 'platt': 40886,\n",
       " 'mcnab': 52159,\n",
       " 'clumsiness': 27643,\n",
       " 'altogether': 3899,\n",
       " 'massacring': 42584,\n",
       " 'bicenntinial': 52160,\n",
       " 'skaal': 40887,\n",
       " 'droning': 14360,\n",
       " 'lds': 8776,\n",
       " 'jaguar': 21912,\n",
       " \"cale's\": 34727,\n",
       " 'nicely': 1777,\n",
       " 'mummy': 4588,\n",
       " \"lot's\": 18513,\n",
       " 'patch': 10086,\n",
       " 'kerkhof': 50202,\n",
       " \"leader's\": 52161,\n",
       " \"'movie\": 27644,\n",
       " 'uncomfirmed': 52162,\n",
       " 'heirloom': 40888,\n",
       " 'wrangle': 47360,\n",
       " 'emotion\\x85': 52163,\n",
       " \"'stargate'\": 52164,\n",
       " 'pinoy': 40889,\n",
       " 'conchatta': 40890,\n",
       " 'broeke': 41128,\n",
       " 'advisedly': 40891,\n",
       " \"barker's\": 17636,\n",
       " 'descours': 52166,\n",
       " 'lots': 772,\n",
       " 'lotr': 9259,\n",
       " 'irs': 9879,\n",
       " 'lott': 52167,\n",
       " 'xvi': 40892,\n",
       " 'irk': 34728,\n",
       " 'irl': 52168,\n",
       " 'ira': 6887,\n",
       " 'belzer': 21913,\n",
       " 'irc': 52169,\n",
       " 'ire': 27645,\n",
       " 'requisites': 40893,\n",
       " 'discipline': 7693,\n",
       " 'lyoko': 52961,\n",
       " 'extend': 11310,\n",
       " 'nature': 873,\n",
       " \"'dickie'\": 52170,\n",
       " 'optimist': 40894,\n",
       " 'lapping': 30586,\n",
       " 'superficial': 3900,\n",
       " 'vestment': 52171,\n",
       " 'extent': 2823,\n",
       " 'tendons': 52172,\n",
       " \"heller's\": 52173,\n",
       " 'quagmires': 52174,\n",
       " 'miyako': 52175,\n",
       " 'moocow': 20601,\n",
       " \"coles'\": 52176,\n",
       " 'lookit': 40895,\n",
       " 'ravenously': 52177,\n",
       " 'levitating': 40896,\n",
       " 'perfunctorily': 52178,\n",
       " 'lookin': 30587,\n",
       " \"lot'\": 40898,\n",
       " 'lookie': 52179,\n",
       " 'fearlessly': 34870,\n",
       " 'libyan': 52181,\n",
       " 'fondles': 40899,\n",
       " 'gopher': 35714,\n",
       " 'wearying': 40901,\n",
       " \"nz's\": 52182,\n",
       " 'minuses': 27646,\n",
       " 'puposelessly': 52183,\n",
       " 'shandling': 52184,\n",
       " 'decapitates': 31268,\n",
       " 'humming': 11929,\n",
       " \"'nother\": 40902,\n",
       " 'smackdown': 21914,\n",
       " 'underdone': 30588,\n",
       " 'frf': 40903,\n",
       " 'triviality': 52185,\n",
       " 'fro': 25248,\n",
       " 'bothers': 8777,\n",
       " \"'kensington\": 52186,\n",
       " 'much': 73,\n",
       " 'muco': 34730,\n",
       " 'wiseguy': 22615,\n",
       " \"richie's\": 27648,\n",
       " 'tonino': 40904,\n",
       " 'unleavened': 52187,\n",
       " 'fry': 11587,\n",
       " \"'tv'\": 40905,\n",
       " 'toning': 40906,\n",
       " 'obese': 14361,\n",
       " 'sensationalized': 30589,\n",
       " 'spiv': 40907,\n",
       " 'spit': 6259,\n",
       " 'arkin': 7364,\n",
       " 'charleton': 21915,\n",
       " 'jeon': 16823,\n",
       " 'boardroom': 21916,\n",
       " 'doubts': 4989,\n",
       " 'spin': 3084,\n",
       " 'hepo': 53083,\n",
       " 'wildcat': 27649,\n",
       " 'venoms': 10584,\n",
       " 'misconstrues': 52191,\n",
       " 'mesmerising': 18514,\n",
       " 'misconstrued': 40908,\n",
       " 'rescinds': 52192,\n",
       " 'prostrate': 52193,\n",
       " 'majid': 40909,\n",
       " 'climbed': 16479,\n",
       " 'canoeing': 34731,\n",
       " 'majin': 52195,\n",
       " 'animie': 57804,\n",
       " 'sylke': 40910,\n",
       " 'conditioned': 14899,\n",
       " 'waddell': 40911,\n",
       " '3\\x85': 52196,\n",
       " 'hyperdrive': 41188,\n",
       " 'conditioner': 34732,\n",
       " 'bricklayer': 53153,\n",
       " 'hong': 2576,\n",
       " 'memoriam': 52198,\n",
       " 'inventively': 30592,\n",
       " \"levant's\": 25249,\n",
       " 'portobello': 20638,\n",
       " 'remand': 52200,\n",
       " 'mummified': 19504,\n",
       " 'honk': 27650,\n",
       " 'spews': 19505,\n",
       " 'visitations': 40912,\n",
       " 'mummifies': 52201,\n",
       " 'cavanaugh': 25250,\n",
       " 'zeon': 23385,\n",
       " \"jungle's\": 40913,\n",
       " 'viertel': 34733,\n",
       " 'frenchmen': 27651,\n",
       " 'torpedoes': 52202,\n",
       " 'schlessinger': 52203,\n",
       " 'torpedoed': 34734,\n",
       " 'blister': 69876,\n",
       " 'cinefest': 52204,\n",
       " 'furlough': 34735,\n",
       " 'mainsequence': 52205,\n",
       " 'mentors': 40914,\n",
       " 'academic': 9094,\n",
       " 'stillness': 20602,\n",
       " 'academia': 40915,\n",
       " 'lonelier': 52206,\n",
       " 'nibby': 52207,\n",
       " \"losers'\": 52208,\n",
       " 'cineastes': 40916,\n",
       " 'corporate': 4449,\n",
       " 'massaging': 40917,\n",
       " 'bellow': 30593,\n",
       " 'absurdities': 19506,\n",
       " 'expetations': 53241,\n",
       " 'nyfiken': 40918,\n",
       " 'mehras': 75638,\n",
       " 'lasse': 52209,\n",
       " 'visability': 52210,\n",
       " 'militarily': 33946,\n",
       " \"elder'\": 52211,\n",
       " 'gainsbourg': 19023,\n",
       " 'hah': 20603,\n",
       " 'hai': 13420,\n",
       " 'haj': 34736,\n",
       " 'hak': 25251,\n",
       " 'hal': 4311,\n",
       " 'ham': 4892,\n",
       " 'duffer': 53259,\n",
       " 'haa': 52213,\n",
       " 'had': 66,\n",
       " 'advancement': 11930,\n",
       " 'hag': 16825,\n",
       " \"hand'\": 25252,\n",
       " 'hay': 13421,\n",
       " 'mcnamara': 20604,\n",
       " \"mozart's\": 52214,\n",
       " 'duffel': 30731,\n",
       " 'haq': 30594,\n",
       " 'har': 13887,\n",
       " 'has': 44,\n",
       " 'hat': 2401,\n",
       " 'hav': 40919,\n",
       " 'haw': 30595,\n",
       " 'figtings': 52215,\n",
       " 'elders': 15495,\n",
       " 'underpanted': 52216,\n",
       " 'pninson': 52217,\n",
       " 'unequivocally': 27652,\n",
       " \"barbara's\": 23673,\n",
       " \"bello'\": 52219,\n",
       " 'indicative': 12997,\n",
       " 'yawnfest': 40920,\n",
       " 'hexploitation': 52220,\n",
       " \"loder's\": 52221,\n",
       " 'sleuthing': 27653,\n",
       " \"justin's\": 32622,\n",
       " \"'ball\": 52222,\n",
       " \"'summer\": 52223,\n",
       " \"'demons'\": 34935,\n",
       " \"mormon's\": 52225,\n",
       " \"laughton's\": 34737,\n",
       " 'debell': 52226,\n",
       " 'shipyard': 39724,\n",
       " 'unabashedly': 30597,\n",
       " 'disks': 40401,\n",
       " 'crowd': 2290,\n",
       " 'crowe': 10087,\n",
       " \"vancouver's\": 56434,\n",
       " 'mosques': 34738,\n",
       " 'crown': 6627,\n",
       " 'culpas': 52227,\n",
       " 'crows': 27654,\n",
       " 'surrell': 53344,\n",
       " 'flowless': 52229,\n",
       " 'sheirk': 52230,\n",
       " \"'three\": 40923,\n",
       " \"peterson'\": 52231,\n",
       " 'ooverall': 52232,\n",
       " 'perchance': 40924,\n",
       " 'bottom': 1321,\n",
       " 'chabert': 53363,\n",
       " 'sneha': 52233,\n",
       " 'inhuman': 13888,\n",
       " 'ichii': 52234,\n",
       " 'ursla': 52235,\n",
       " 'completly': 30598,\n",
       " 'moviedom': 40925,\n",
       " 'raddick': 52236,\n",
       " 'brundage': 51995,\n",
       " 'brigades': 40926,\n",
       " 'starring': 1181,\n",
       " \"'goal'\": 52237,\n",
       " 'caskets': 52238,\n",
       " 'willcock': 52239,\n",
       " \"threesome's\": 52240,\n",
       " \"mosque'\": 52241,\n",
       " \"cover's\": 52242,\n",
       " 'spaceships': 17637,\n",
       " 'anomalous': 40927,\n",
       " 'ptsd': 27655,\n",
       " 'shirdan': 52243,\n",
       " 'obscenity': 21962,\n",
       " 'lemmings': 30599,\n",
       " 'duccio': 30600,\n",
       " \"levene's\": 52244,\n",
       " \"'gorby'\": 52245,\n",
       " \"teenager's\": 25255,\n",
       " 'marshall': 5340,\n",
       " 'honeymoon': 9095,\n",
       " 'shoots': 3231,\n",
       " 'despised': 12258,\n",
       " 'okabasho': 52246,\n",
       " 'fabric': 8289,\n",
       " 'cannavale': 18515,\n",
       " 'raped': 3537,\n",
       " \"tutt's\": 52247,\n",
       " 'grasping': 17638,\n",
       " 'despises': 18516,\n",
       " \"thief's\": 40928,\n",
       " 'rapes': 8926,\n",
       " 'raper': 52248,\n",
       " \"eyre'\": 27656,\n",
       " 'walchek': 52249,\n",
       " \"elmo's\": 23386,\n",
       " 'perfumes': 40929,\n",
       " 'spurting': 21918,\n",
       " \"exposition'\\x85\": 52250,\n",
       " 'denoting': 52251,\n",
       " 'thesaurus': 34740,\n",
       " \"shoot'\": 40930,\n",
       " 'bonejack': 49759,\n",
       " 'simpsonian': 52253,\n",
       " 'hebetude': 30601,\n",
       " \"hallow's\": 34741,\n",
       " 'desperation\\x85': 52254,\n",
       " 'incinerator': 34742,\n",
       " 'congratulations': 10308,\n",
       " 'humbled': 52255,\n",
       " \"else's\": 5924,\n",
       " 'trelkovski': 40845,\n",
       " \"rape'\": 52256,\n",
       " \"'chapters'\": 59386,\n",
       " '1600s': 52257,\n",
       " 'martian': 7253,\n",
       " 'nicest': 25256,\n",
       " 'eyred': 52259,\n",
       " 'passenger': 9457,\n",
       " 'disgrace': 6041,\n",
       " 'moderne': 52260,\n",
       " 'barrymore': 5120,\n",
       " 'yankovich': 52261,\n",
       " 'moderns': 40931,\n",
       " 'studliest': 52262,\n",
       " 'bedsheet': 52263,\n",
       " 'decapitation': 14900,\n",
       " 'slurring': 52264,\n",
       " \"'nunsploitation'\": 52265,\n",
       " \"'character'\": 34743,\n",
       " 'cambodia': 9880,\n",
       " 'rebelious': 52266,\n",
       " 'pasadena': 27657,\n",
       " 'crowne': 40932,\n",
       " \"'bedchamber\": 52267,\n",
       " 'conjectural': 52268,\n",
       " 'appologize': 52269,\n",
       " 'halfassing': 52270,\n",
       " 'paycheque': 57816,\n",
       " 'palms': 20606,\n",
       " \"'islands\": 52271,\n",
       " 'hawked': 40933,\n",
       " 'palme': 21919,\n",
       " 'conservatively': 40934,\n",
       " 'larp': 64007,\n",
       " 'palma': 5558,\n",
       " 'smelling': 21920,\n",
       " 'aragorn': 12998,\n",
       " 'hawker': 52272,\n",
       " 'hawkes': 52273,\n",
       " 'explosions': 3975,\n",
       " 'loren': 8059,\n",
       " \"pyle's\": 52274,\n",
       " 'shootout': 6704,\n",
       " \"mike's\": 18517,\n",
       " \"driscoll's\": 52275,\n",
       " 'cogsworth': 40935,\n",
       " \"britian's\": 52276,\n",
       " 'childs': 34744,\n",
       " \"portrait's\": 52277,\n",
       " 'chain': 3626,\n",
       " 'whoever': 2497,\n",
       " 'puttered': 52278,\n",
       " 'childe': 52279,\n",
       " 'maywether': 52280,\n",
       " 'chair': 3036,\n",
       " \"rance's\": 52281,\n",
       " 'machu': 34745,\n",
       " 'ballet': 4517,\n",
       " 'grapples': 34746,\n",
       " 'summerize': 76152,\n",
       " 'freelance': 30603,\n",
       " \"andrea's\": 52283,\n",
       " '\\x91very': 52284,\n",
       " 'coolidge': 45879,\n",
       " 'mache': 18518,\n",
       " 'balled': 52285,\n",
       " 'grappled': 40937,\n",
       " 'macha': 18519,\n",
       " 'underlining': 21921,\n",
       " 'macho': 5623,\n",
       " 'oversight': 19507,\n",
       " 'machi': 25257,\n",
       " 'verbally': 11311,\n",
       " 'tenacious': 21922,\n",
       " 'windshields': 40938,\n",
       " 'paychecks': 18557,\n",
       " 'jerk': 3396,\n",
       " \"good'\": 11931,\n",
       " 'prancer': 34748,\n",
       " 'prances': 21923,\n",
       " 'olympus': 52286,\n",
       " 'lark': 21924,\n",
       " 'embark': 10785,\n",
       " 'gloomy': 7365,\n",
       " 'jehaan': 52287,\n",
       " 'turaqui': 52288,\n",
       " \"child'\": 20607,\n",
       " 'locked': 2894,\n",
       " 'pranced': 52289,\n",
       " 'exact': 2588,\n",
       " 'unattuned': 52290,\n",
       " 'minute': 783,\n",
       " 'skewed': 16118,\n",
       " 'hodgins': 40940,\n",
       " 'skewer': 34749,\n",
       " 'think\\x85': 52291,\n",
       " 'rosenstein': 38765,\n",
       " 'helmit': 52292,\n",
       " 'wrestlemanias': 34750,\n",
       " 'hindered': 16826,\n",
       " \"martha's\": 30604,\n",
       " 'cheree': 52293,\n",
       " \"pluckin'\": 52294,\n",
       " 'ogles': 40941,\n",
       " 'heavyweight': 11932,\n",
       " 'aada': 82190,\n",
       " 'chopping': 11312,\n",
       " 'strongboy': 61534,\n",
       " 'hegemonic': 41342,\n",
       " 'adorns': 40942,\n",
       " 'xxth': 41346,\n",
       " 'nobuhiro': 34751,\n",
       " 'capitÃ£es': 52298,\n",
       " 'kavogianni': 52299,\n",
       " 'antwerp': 13422,\n",
       " 'celebrated': 6538,\n",
       " 'roarke': 52300,\n",
       " 'baggins': 40943,\n",
       " 'cheeseburgers': 31270,\n",
       " 'matras': 52301,\n",
       " \"nineties'\": 52302,\n",
       " \"'craig'\": 52303,\n",
       " 'celebrates': 12999,\n",
       " 'unintentionally': 3383,\n",
       " 'drafted': 14362,\n",
       " 'climby': 52304,\n",
       " '303': 52305,\n",
       " 'oldies': 18520,\n",
       " 'climbs': 9096,\n",
       " 'honour': 9655,\n",
       " 'plucking': 34752,\n",
       " '305': 30074,\n",
       " 'address': 5514,\n",
       " 'menjou': 40944,\n",
       " \"'freak'\": 42592,\n",
       " 'dwindling': 19508,\n",
       " 'benson': 9458,\n",
       " 'whiteâ€™s': 52307,\n",
       " 'shamelessness': 40945,\n",
       " 'impacted': 21925,\n",
       " 'upatz': 52308,\n",
       " 'cusack': 3840,\n",
       " \"flavia's\": 37567,\n",
       " 'effette': 52309,\n",
       " 'influx': 34753,\n",
       " 'boooooooo': 52310,\n",
       " 'dimitrova': 52311,\n",
       " 'houseman': 13423,\n",
       " 'bigas': 25259,\n",
       " 'boylen': 52312,\n",
       " 'phillipenes': 52313,\n",
       " 'fakery': 40946,\n",
       " \"grandpa's\": 27658,\n",
       " 'darnell': 27659,\n",
       " 'undergone': 19509,\n",
       " 'handbags': 52315,\n",
       " 'perished': 21926,\n",
       " 'pooped': 37778,\n",
       " 'vigour': 27660,\n",
       " 'opposed': 3627,\n",
       " 'etude': 52316,\n",
       " \"caine's\": 11799,\n",
       " 'doozers': 52317,\n",
       " 'photojournals': 34754,\n",
       " 'perishes': 52318,\n",
       " 'constrains': 34755,\n",
       " 'migenes': 40948,\n",
       " 'consoled': 30605,\n",
       " 'alastair': 16827,\n",
       " 'wvs': 52319,\n",
       " 'ooooooh': 52320,\n",
       " 'approving': 34756,\n",
       " 'consoles': 40949,\n",
       " 'disparagement': 52064,\n",
       " 'futureistic': 52322,\n",
       " 'rebounding': 52323,\n",
       " \"'date\": 52324,\n",
       " 'gregoire': 52325,\n",
       " 'rutherford': 21927,\n",
       " 'americanised': 34757,\n",
       " 'novikov': 82196,\n",
       " 'following': 1042,\n",
       " 'munroe': 34758,\n",
       " \"morita'\": 52326,\n",
       " 'christenssen': 52327,\n",
       " 'oatmeal': 23106,\n",
       " 'fossey': 25260,\n",
       " 'livered': 40950,\n",
       " 'listens': 13000,\n",
       " \"'marci\": 76164,\n",
       " \"otis's\": 52330,\n",
       " 'thanking': 23387,\n",
       " 'maude': 16019,\n",
       " 'extensions': 34759,\n",
       " 'ameteurish': 52332,\n",
       " \"commender's\": 52333,\n",
       " 'agricultural': 27661,\n",
       " 'convincingly': 4518,\n",
       " 'fueled': 17639,\n",
       " 'mahattan': 54014,\n",
       " \"paris's\": 40952,\n",
       " 'vulkan': 52336,\n",
       " 'stapes': 52337,\n",
       " 'odysessy': 52338,\n",
       " 'harmon': 12259,\n",
       " 'surfing': 4252,\n",
       " 'halloran': 23494,\n",
       " 'unbelieveably': 49580,\n",
       " \"'offed'\": 52339,\n",
       " 'quadrant': 30607,\n",
       " 'inhabiting': 19510,\n",
       " 'nebbish': 34760,\n",
       " 'forebears': 40953,\n",
       " 'skirmish': 34761,\n",
       " 'ocassionally': 52340,\n",
       " \"'resist\": 52341,\n",
       " 'impactful': 21928,\n",
       " 'spicier': 52342,\n",
       " 'touristy': 40954,\n",
       " \"'football'\": 52343,\n",
       " 'webpage': 40955,\n",
       " 'exurbia': 52345,\n",
       " 'jucier': 52346,\n",
       " 'professors': 14901,\n",
       " 'structuring': 34762,\n",
       " 'jig': 30608,\n",
       " 'overlord': 40956,\n",
       " 'disconnect': 25261,\n",
       " 'sniffle': 82201,\n",
       " 'slimeball': 40957,\n",
       " 'jia': 40958,\n",
       " 'milked': 16828,\n",
       " 'banjoes': 40959,\n",
       " 'jim': 1237,\n",
       " 'workforces': 52348,\n",
       " 'jip': 52349,\n",
       " 'rotweiller': 52350,\n",
       " 'mundaneness': 34763,\n",
       " \"'ninja'\": 52351,\n",
       " \"dead'\": 11040,\n",
       " \"cipriani's\": 40960,\n",
       " 'modestly': 20608,\n",
       " \"professor'\": 52352,\n",
       " 'shacked': 40961,\n",
       " 'bashful': 34764,\n",
       " 'sorter': 23388,\n",
       " 'overpowering': 16120,\n",
       " 'workmanlike': 18521,\n",
       " 'henpecked': 27662,\n",
       " 'sorted': 18522,\n",
       " \"jÅb's\": 52354,\n",
       " \"'always\": 52355,\n",
       " \"'baptists\": 34765,\n",
       " 'dreamcatchers': 52356,\n",
       " \"'silence'\": 52357,\n",
       " 'hickory': 21929,\n",
       " 'fun\\x97yet': 52358,\n",
       " 'breakumentary': 52359,\n",
       " 'didn': 15496,\n",
       " 'didi': 52360,\n",
       " 'pealing': 52361,\n",
       " 'dispite': 40962,\n",
       " \"italy's\": 25262,\n",
       " 'instability': 21930,\n",
       " 'quarter': 6539,\n",
       " 'quartet': 12608,\n",
       " 'padmÃ©': 52362,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'pahalniuk': 52364,\n",
       " 'honduras': 52365,\n",
       " 'bursting': 10786,\n",
       " \"pablo's\": 41465,\n",
       " 'irremediably': 52367,\n",
       " 'presages': 40963,\n",
       " 'bowlegged': 57832,\n",
       " 'dalip': 65183,\n",
       " 'entering': 6260,\n",
       " 'newsradio': 76172,\n",
       " 'presaged': 54150,\n",
       " \"giallo's\": 27663,\n",
       " 'bouyant': 40964,\n",
       " 'amerterish': 52368,\n",
       " 'rajni': 18523,\n",
       " 'leeves': 30610,\n",
       " 'macauley': 34767,\n",
       " 'seriously': 612,\n",
       " 'sugercoma': 52369,\n",
       " 'grimstead': 52370,\n",
       " \"'fairy'\": 52371,\n",
       " 'zenda': 30611,\n",
       " \"'twins'\": 52372,\n",
       " 'realisation': 17640,\n",
       " 'highsmith': 27664,\n",
       " 'raunchy': 7817,\n",
       " 'incentives': 40965,\n",
       " 'flatson': 52374,\n",
       " 'snooker': 35097,\n",
       " 'crazies': 16829,\n",
       " 'crazier': 14902,\n",
       " 'grandma': 7094,\n",
       " 'napunsaktha': 52375,\n",
       " 'workmanship': 30612,\n",
       " 'reisner': 52376,\n",
       " \"sanford's\": 61306,\n",
       " '\\x91doÃ±a': 52377,\n",
       " 'modest': 6108,\n",
       " \"everything's\": 19153,\n",
       " 'hamer': 40966,\n",
       " \"couldn't'\": 52379,\n",
       " 'quibble': 13001,\n",
       " 'socking': 52380,\n",
       " 'tingler': 21931,\n",
       " 'gutman': 52381,\n",
       " 'lachlan': 40967,\n",
       " 'tableaus': 52382,\n",
       " 'headbanger': 52383,\n",
       " 'spoken': 2847,\n",
       " 'cerebrally': 34768,\n",
       " \"'road\": 23490,\n",
       " 'tableaux': 21932,\n",
       " \"proust's\": 40968,\n",
       " 'periodical': 40969,\n",
       " \"shoveller's\": 52385,\n",
       " 'tamara': 25263,\n",
       " 'affords': 17641,\n",
       " 'concert': 3249,\n",
       " \"yara's\": 87955,\n",
       " 'someome': 52386,\n",
       " 'lingering': 8424,\n",
       " \"abraham's\": 41511,\n",
       " 'beesley': 34769,\n",
       " 'cherbourg': 34770,\n",
       " 'kagan': 28624,\n",
       " 'snatch': 9097,\n",
       " \"miyazaki's\": 9260,\n",
       " 'absorbs': 25264,\n",
       " \"koltai's\": 40970,\n",
       " 'tingled': 64027,\n",
       " 'crossroads': 19511,\n",
       " 'rehab': 16121,\n",
       " 'falworth': 52389,\n",
       " 'sequals': 52390,\n",
       " ...}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the word index\n",
    "word_index = tf.keras.datasets.imdb.get_word_index() ## dict with word as key, and index as value\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db0cd3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## what about decoding the first review in X_train_full\n",
    "## well, but note \n",
    "## that integers(0,1,2) is reserved for \n",
    "## (padding <pad>, start of sequence <sos>, unknown words <unk>)\n",
    "\n",
    "reversed_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "## get the first review, join with space and decrease the value of dict by 3 for reserved words\n",
    "decoded_first_review = ' '.join([reversed_word_index.get(i-3, '?') for i in X_train_full[0]])\n",
    "decoded_first_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1e3b1",
   "metadata": {},
   "source": [
    "* `the above data is already preprocessed , what about preprocessing it ourselves`\n",
    "* `load the real data and let make our hands dirty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37f8d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the original dataset\n",
    "imdb_dataset, imdb_info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d75be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_train => 25000\n",
      "size_test => 25000\n"
     ]
    }
   ],
   "source": [
    "size_train = imdb_info.splits['train'].num_examples\n",
    "size_test = imdb_info.splits['test'].num_examples\n",
    "\n",
    "print('size_train =>', size_train)\n",
    "print('size_test =>', size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8c44e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review is => This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ....\n",
      "Label is => 0 ==> Negative\n",
      "\n",
      "Review is => I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ....\n",
      "Label is => 0 ==> Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## showing sample\n",
    "for X, y in imdb_dataset['train'].batch(1).take(2):\n",
    "    for review, label in zip(X.numpy(), y.numpy()):\n",
    "        print('Review is =>', review.decode('utf-8')[:200], '....')\n",
    "        print('Label is =>', label, '==> Negative' if label==0 else '==> Positive')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4183781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is to prepare to craete the lookup table\n",
    "def preprocess_imdb(X_batch, y_batch):\n",
    "    ## take only the first 300 char (enough for sentiement analysis)\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)  \n",
    "    ## replace the <br /> with spaces\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\s*/?>\", b\" \")\n",
    "    ## replace any char not in a-z or A-Z with spaces\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    ## split for each word\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    ## convert this ragged tensor to dense one, padding all reviews to have the same lenght\n",
    "    X_batch = X_batch.to_tensor(default_value=b\"<pad>\")\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8e785f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we must loop aver the whole datset and get each word count\n",
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "for X_batch, y_batch in  imdb_dataset['train'].batch(32).map(preprocess_imdb):\n",
    "    for review in X_batch:\n",
    "        # add elements from another iterable\n",
    "        vocab.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "36dff4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c0e573fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can truncate the vocab and take only the max 10000 word\n",
    "vocab_size = 10000\n",
    "truncated_vocab = [word for word, count in vocab.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3c5c2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "## map each word for its ID ==> Create a lookup table\n",
    "words = tf.constant(truncated_vocab)\n",
    "words_idxs = tf.range(len(truncated_vocab), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be0927",
   "metadata": {},
   "source": [
    "#### `What is lookup table`\n",
    "\n",
    "``` python\n",
    "        init = tf.lookup.KeyValueTensorInitializer(\n",
    "                    keys=tf.constant(['emerson', 'lake', 'palmer']),\n",
    "                    values=tf.constant([0, 1, 2], dtype=tf.int64))\n",
    "        table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets=5)\n",
    "\n",
    "```\n",
    "The `Vocabulary` object will performs the following mapping:\n",
    "* `emerson -> 0`\n",
    "* `lake -> 1`\n",
    "* `palmer -> 2`\n",
    "* `<other term> -> bucket_id`, where `bucket_id` will be between `3` and\n",
    "`3 + num_oov_buckets - 1 = 7`, calculated by:\n",
    "`hash(<term>) % num_oov_buckets + vocab_size`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07f97373",
   "metadata": {},
   "outputs": [],
   "source": [
    "## init lookup\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, words_idxs)\n",
    "\n",
    "## use oov (out of vocab) (you already know what it is)\n",
    "use_oov = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets=use_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fe528831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    9,    15,     7,    34, 10703]], dtype=int64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test your table\n",
    "\n",
    "txt_test = 'this film is so fantaaaaastic'\n",
    "table.lookup(tf.constant([txt_test.split()])).numpy() \n",
    "## note that for all words excepth the final one do not exceed the max size(>10000)\n",
    "## but the final word is out of the vocabulary so the index is >10000\n",
    "## so it eas mapped to one of the oov buckets with an ID integer greater or equal than 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec025113",
   "metadata": {},
   "outputs": [],
   "source": [
    "## finally Indexing the whole text\n",
    "def encode_text(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e813c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_imdb = imdb_dataset['train'].batch(32).map(preprocess_imdb).map(encode_text)\n",
    "train_set_imdb = train_set_imdb.prefetch(1)  #### Training the Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f20654f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[  22   11   28 ...    0    0    0]\n",
      "  [   6   21   70 ...    0    0    0]\n",
      "  [4099 6881    1 ...    0    0    0]\n",
      "  ...\n",
      "  [  22   12  118 ...  331 1047    0]\n",
      "  [1757 4101  451 ...    0    0    0]\n",
      "  [3365 4392    6 ...    0    0    0]]], shape=(1, 32, 60), dtype=int64)\n",
      "\n",
      "tf.Tensor([[0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0]], shape=(1, 32), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "## show our final step here, what we catch till here\n",
    "for X, y in train_set_imdb.batch(1).take(1):\n",
    "    print(X) ## 32 review each with 60 word (aroud 300 chars we cut before)\n",
    "    print()\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aedd57",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "16727b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, use_oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "893bd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_embed_dims = 128  ## should be tuned\n",
    "\n",
    "model_imdb = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size+use_oov, \n",
    "                              output_dim=try_embed_dims, input_shape=[None]),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8ccf8643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 23s 26ms/step - loss: 0.6042 - accuracy: 0.6384\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.3873 - accuracy: 0.8327\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 19s 25ms/step - loss: 0.2423 - accuracy: 0.9099\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.1616 - accuracy: 0.9430\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.1278 - accuracy: 0.9539\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.0928 - accuracy: 0.9679\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0703 - accuracy: 0.9764\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0581 - accuracy: 0.9805\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0473 - accuracy: 0.9840\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.0444 - accuracy: 0.9853\n"
     ]
    }
   ],
   "source": [
    "model_imdb.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history =  model_imdb.fit(train_set_imdb, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6a8301bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imdb.save('model_imdb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7495a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
